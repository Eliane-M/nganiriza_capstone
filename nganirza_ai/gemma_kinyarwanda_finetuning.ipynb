{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma for Kinyarwanda: Sex Education & Mental Health for Young Girls\n",
    "\n",
    "This notebook fine-tunes the Gemma model for Kinyarwanda to help young girls with:\n",
    "- **Sex Education**: Age-appropriate, culturally sensitive information about reproductive health\n",
    "- **Self-Protection**: Guidance on how to protect themselves and behave safely in society\n",
    "- **Mental Health Awareness**: Counseling and support for mental well-being\n",
    "\n",
    "## Overview\n",
    "- Load pre-created Kinyarwanda instruction-response pairs from local JSONL files\n",
    "- Fine-tune Gemma model on the dataset\n",
    "- Focus on empowering young girls with knowledge, safety awareness, and mental health support\n",
    "- Culturally appropriate and age-appropriate content in Kinyarwanda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes peft datasets\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Local Counseling Dataset (JSONL)\n",
    "\n",
    "Load the pre-created Kinyarwanda counseling dataset from JSONL files in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to data directory\n",
    "data_dir = Path(\"./data\")\n",
    "\n",
    "# Load training and validation datasets from JSONL files\n",
    "train_file = data_dir / \"kinyarwanda_counseling.train.jsonl\"\n",
    "val_file = data_dir / \"kinyarwanda_counseling.validation.jsonl\"\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load data from JSONL file.\"\"\"\n",
    "    data = []\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading Kinyarwanda counseling dataset from JSONL files...\")\n",
    "train_data = load_jsonl(train_file)\n",
    "val_data = load_jsonl(val_file)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training pairs\")\n",
    "print(f\"Loaded {len(val_data)} validation pairs\")\n",
    "print(f\"Total pairs: {len(train_data) + len(val_data)}\")\n",
    "\n",
    "# Extract instructions and responses\n",
    "kinyarwanda_instructions = [item[\"instruction\"] for item in train_data + val_data]\n",
    "kinyarwanda_responses = [item[\"output\"] for item in train_data + val_data]\n",
    "\n",
    "# Display sample generated content\n",
    "print(\"\\n=== Sample Generated Content ===\")\n",
    "for i in range(min(3, len(train_data))):\n",
    "    record = train_data[i]\n",
    "    print(f\"\\nTopic: {record['topic']}\")\n",
    "    print(f\"Scenario: {record['scenario']}\")\n",
    "    print(f\"\\nInstruction: {record['instruction']}\")\n",
    "    print(f\"\\nResponse: {record['output'][:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Per-topic statistics\n",
    "from collections import Counter\n",
    "topic_counts = Counter(item[\"topic\"] for item in train_data + val_data)\n",
    "print(\"\\n=== Dataset Statistics ===\")\n",
    "print(f\"Total topics: {len(topic_counts)}\")\n",
    "print(f\"Pairs per topic: {dict(topic_counts)}\")\n",
    "print(f\"\\nDomain distribution:\")\n",
    "domain_counts = Counter(item[\"domain\"] for item in train_data + val_data)\n",
    "print(dict(domain_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset\n",
    "\n",
    "The dataset is saved in JSONL format in the `data/` directory:\n",
    "- `data/kinyarwanda_counseling.train.jsonl` (training set)\n",
    "- `data/kinyarwanda_counseling.validation.jsonl` (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset is saved in JSONL format in the data/ directory.\")\n",
    "print(\"Training data: data/kinyarwanda_counseling.train.jsonl\")\n",
    "print(\"Validation data: data/kinyarwanda_counseling.validation.jsonl\")\n",
    "print(f\"Total pairs loaded: {len(kinyarwanda_instructions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format Data for Gemma Model\n",
    "\n",
    "Gemma uses a specific chat template format with `<start_of_turn>` and `<end_of_turn>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gemma_instruction(instruction, response):\n",
    "    \"\"\"\n",
    "    Format instruction and response in Gemma chat template format.\n",
    "    \n",
    "    Args:\n",
    "        instruction: User instruction\n",
    "        response: Model response\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string for training\n",
    "    \"\"\"\n",
    "    formatted = f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\"\n",
    "    return formatted\n",
    "\n",
    "# Create formatted training examples\n",
    "training_examples = []\n",
    "for inst, resp in zip(kinyarwanda_instructions, kinyarwanda_responses):\n",
    "    formatted = format_gemma_instruction(inst, resp)\n",
    "    training_examples.append(formatted)\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")\n",
    "print(\"\\n=== Sample Formatted Example ===\")\n",
    "print(training_examples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from the training examples\n",
    "dataset_dict = {\n",
    "    \"text\": training_examples\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} examples\")\n",
    "print(f\"Sample entry:\\n{dataset[0]['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Gemma-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"google/gemma-2-2b-it\"  # Using Gemma-2 2B instruction-tuned model\n",
    "# Alternative: \"google/gemma-2-9b-it\" for larger model (requires more memory)\n",
    "\n",
    "login()\n",
    "\n",
    "# Configure quantization for memory efficiency (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure PEFT (Parameter-Efficient Fine-Tuning) with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha scaling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text examples for training.\n",
    "    \"\"\"\n",
    "    # Tokenize with truncation and padding\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset created with {len(tokenized_dataset)} examples\")\n",
    "print(f\"Sample tokenized keys: {tokenized_dataset[0].keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-kinyarwanda-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,  # Use bfloat16 instead\n",
    "    bf16=True,   # Better for Gemma models\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Initialize Trainer and Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(f\"Training on {len(tokenized_dataset)} examples\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nFine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./gemma-kinyarwanda-finetuned-final\"\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load fine-tuned LoRA weights\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded for inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Generate Responses in Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate a response using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt in Kinyarwanda\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    # Format prompt in Gemma chat format\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    if \"<start_of_turn>model\\n\" in full_response:\n",
    "        response = full_response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
    "    else:\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with sample prompts from the loaded dataset\n",
    "test_prompts = kinyarwanda_instructions[:3]\n",
    "\n",
    "print(\"=== Testing Fine-tuned Model ===\\n\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Compare with Base Model (Optional)\n",
    "\n",
    "Compare responses from the base model vs fine-tuned model to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model for comparison\n",
    "base_model_for_comparison = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "def generate_base_response(prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"Generate response using base model.\"\"\"\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(base_model_for_comparison.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model_for_comparison.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<start_of_turn>model\\n\" in full_response:\n",
    "        response = full_response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
    "    else:\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Compare responses with a prompt from the dataset\n",
    "test_prompt = kinyarwanda_instructions[0]\n",
    "\n",
    "print(\"=== Model Comparison ===\\n\")\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "\n",
    "print(\"Base Model Response:\")\n",
    "base_response = generate_base_response(test_prompt)\n",
    "print(base_response)\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"Fine-tuned Model Response:\")\n",
    "fine_tuned_response = generate_response(test_prompt)\n",
    "print(fine_tuned_response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Save Model to Google Drive (Optional)\n",
    "\n",
    "If running on Google Colab and want to save the model to Google Drive for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally first\n",
    "print(f\"âœ… Model saved locally at: {output_dir}\")\n",
    "print(\"\\nNext: We'll merge LoRA adapters and convert to GGUF format for Ollama deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Tips\n",
    "\n",
    "1. **Dataset**:\n",
    "   - The dataset is pre-created and stored in `data/kinyarwanda_counseling.train.jsonl` and `data/kinyarwanda_counseling.validation.jsonl`\n",
    "   - Content covers sex education, self-protection, and mental health topics for young girls (ages 10-16)\n",
    "   - All content is in natural Kinyarwanda, culturally sensitive, and age-appropriate\n",
    "\n",
    "2. **Memory Management**: If you run out of memory, try:\n",
    "   - Reducing `per_device_train_batch_size`\n",
    "   - Increasing `gradient_accumulation_steps`\n",
    "   - Using a smaller model (e.g., gemma-2-2b instead of 9b)\n",
    "\n",
    "3. **Training Time**: \n",
    "   - Fine-tuning typically takes 10-30 minutes depending on dataset size and hardware\n",
    "   - GGUF conversion adds another 5-10 minutes\n",
    "\n",
    "4. **GGUF Conversion**:\n",
    "   - Q4_K_M quantization provides good balance between size and quality\n",
    "   - Other options: Q2_K (smaller, lower quality), Q5_K_M (larger, higher quality), Q8_0 (largest, best quality)\n",
    "   - Final GGUF file will be significantly smaller than the merged model\n",
    "\n",
    "5. **Evaluation**: \n",
    "   - Test with real scenarios that young girls might ask about\n",
    "   - Evaluate responses for cultural appropriateness, accuracy, and empathy\n",
    "\n",
    "6. **Model Deployment**: \n",
    "   - Use with Ollama for easy local deployment\n",
    "   - Can also deploy on servers or integrate with applications\n",
    "   - Ensure proper disclaimers about limitations\n",
    "\n",
    "7. **Important Disclaimers and Safety Considerations**: \n",
    "   - This model provides educational information only and is NOT a replacement for professional services\n",
    "   - Always direct users to trusted adults and professionals when appropriate\n",
    "   - Include disclaimers in any production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"ðŸŽ‰ \" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ MODEL FINE-TUNING AND CONVERSION COMPLETE!\")\n",
    "print(\"ðŸŽ‰ \" + \"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“ Files Created:\\n\")\n",
    "\n",
    "# Check and display files\n",
    "files_to_check = [\n",
    "    (\"gemma-kinyarwanda.gguf\", \"GGUF model file for Ollama\"),\n",
    "    (\"Modelfile\", \"Ollama configuration file\"),\n",
    "    (\"ollama_usage_instructions.md\", \"Usage instructions\"),\n",
    "]\n",
    "\n",
    "for filename, description in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        if size > 1024*1024*1024:\n",
    "            size_str = f\"{size / (1024**3):.2f} GB\"\n",
    "        elif size > 1024*1024:\n",
    "            size_str = f\"{size / (1024**2):.2f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size / 1024:.2f} KB\"\n",
    "        print(f\"  âœ… {filename:<35} ({size_str}) - {description}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {filename:<35} - NOT FOUND\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\\n\")\n",
    "print(\"1. Install Ollama: https://ollama.ai\")\n",
    "print(\"2. Run: ollama create gemma-kinyarwanda -f Modelfile\")\n",
    "print(\"3. Test: ollama run gemma-kinyarwanda\")\n",
    "print(\"4. Try a query: ollama run gemma-kinyarwanda \\\"Amaraso yanjye yatangiye kuza, ni ibisanzwe?\\\"\")\n",
    "\n",
    "print(\"\\nðŸ“– See 'ollama_usage_instructions.md' for detailed guidance\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Model Purpose:\")\n",
    "print(\"   â€¢ Provides culturally appropriate guidance in Kinyarwanda\")\n",
    "print(\"   â€¢ Covers sex education, self-protection, and mental health\")\n",
    "print(\"   â€¢ Designed for young girls (ages 10-16) in rural Rwanda\")\n",
    "print(\"   â€¢ Empowers girls with knowledge and support\")\n",
    "\n",
    "print(\"\\nâš ï¸  Important Reminders:\")\n",
    "print(\"   â€¢ This model provides educational information only\")\n",
    "print(\"   â€¢ Always consult professionals for personal health concerns\")\n",
    "print(\"   â€¢ Encourage seeking help from trusted adults when needed\")\n",
    "\n",
    "print(\"\\nâœ… Your Gemma Kinyarwanda counseling model is ready for deployment!\")\n",
    "print(\"ðŸ’ª Empowering young girls with knowledge in their native language.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create usage instructions\n",
    "usage_content = \"\"\"# Using Your Gemma Kinyarwanda Model with Ollama\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama installed on your system ([https://ollama.ai](https://ollama.ai))\n",
    "- GGUF file and Modelfile in the same directory\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1. Create the Ollama Model\n",
    "```bash\n",
    "ollama create gemma-kinyarwanda -f Modelfile\n",
    "```\n",
    "\n",
    "### 2. Run the Model\n",
    "```bash\n",
    "# Start interactive chat\n",
    "ollama run gemma-kinyarwanda\n",
    "\n",
    "# Or run a single query\n",
    "ollama run gemma-kinyarwanda \"Amaraso yanjye yatangiye kuza, ni ibisanzwe?\"\n",
    "```\n",
    "\n",
    "## Example Queries in Kinyarwanda\n",
    "\n",
    "### Sex Education\n",
    "```bash\n",
    "ollama run gemma-kinyarwanda \"Ni iki cyo gukora igihe mfite amaraso bwa mbere?\"\n",
    "ollama run gemma-kinyarwanda \"Umubiri wanjye urahinduka, iki ni ibisanzwe?\"\n",
    "```\n",
    "\n",
    "### Mental Health\n",
    "```bash\n",
    "ollama run gemma-kinyarwanda \"Numva ntekereza cyane, nshobora gukora iki?\"\n",
    "ollama run gemma-kinyarwanda \"Ndi mu ngorane, nshobora kubwira nde?\"\n",
    "```\n",
    "\n",
    "### Self-Protection\n",
    "```bash\n",
    "ollama run gemma-kinyarwanda \"Umuntu arantumbagiza, nshobora gukora iki?\"\n",
    "ollama run gemma-kinyarwanda \"Ni gute nshobora kwirinda ku murongo?\"\n",
    "```\n",
    "\n",
    "## Model Information\n",
    "\n",
    "- **Base Model**: google/gemma-2-2b-it\n",
    "- **Quantization**: Q4_K_M\n",
    "- **Language**: Kinyarwanda\n",
    "- **Domain**: Sex education, self-protection, mental health\n",
    "- **Target Audience**: Young girls (ages 10-16) in Rwanda\n",
    "\n",
    "## Important Disclaimer\n",
    "\n",
    "âš ï¸ **Medical Disclaimer**: This model provides educational information only and is NOT a replacement for:\n",
    "- Professional medical or mental health services\n",
    "- Professional counseling or therapy\n",
    "- Emergency services\n",
    "\n",
    "Always consult qualified professionals for personal health concerns.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Model Not Found\n",
    "```bash\n",
    "ollama list  # Check if model is created\n",
    "ollama rm gemma-kinyarwanda  # Remove if needed\n",
    "ollama create gemma-kinyarwanda -f Modelfile  # Recreate\n",
    "```\n",
    "\n",
    "### Performance Issues\n",
    "```bash\n",
    "# Reduce context window if memory is limited\n",
    "ollama run gemma-kinyarwanda --num-ctx 1024\n",
    "```\n",
    "\n",
    "## Files Included\n",
    "\n",
    "- `gemma-kinyarwanda.gguf` - The model file (required)\n",
    "- `Modelfile` - Ollama configuration (required)\n",
    "- `ollama_usage_instructions.md` - This file\n",
    "\"\"\"\n",
    "\n",
    "# Save usage instructions\n",
    "usage_path = \"./ollama_usage_instructions.md\"\n",
    "with open(usage_path, 'w') as f:\n",
    "    f.write(usage_content)\n",
    "\n",
    "print(f\"âœ… Usage instructions created: {usage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Create Usage Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelfile for Ollama\n",
    "modelfile_content = \"\"\"# Modelfile for Gemma Kinyarwanda Counseling Model\n",
    "# Fine-tuned for sex education, self-protection, and mental health counseling\n",
    "\n",
    "FROM ./gemma-kinyarwanda.gguf\n",
    "\n",
    "# Model parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.95\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 2048\n",
    "PARAMETER repeat_penalty 1.1\n",
    "PARAMETER repeat_last_n 64\n",
    "\n",
    "# System prompt for Kinyarwanda counseling\n",
    "SYSTEM \\\"\\\"\\\"You are a helpful, culturally sensitive assistant in Kinyarwanda that provides age-appropriate guidance for young girls (ages 10-16) on:\n",
    "\n",
    "- Sex education and reproductive health (medically accurate, age-appropriate)\n",
    "- Self-protection and personal safety\n",
    "- Mental health awareness and counseling\n",
    "- Social behavior and healthy relationships\n",
    "- Body awareness, consent, and boundaries\n",
    "\n",
    "IMPORTANT GUIDELINES:\n",
    "- Always respond in natural, fluent Kinyarwanda\n",
    "- Be culturally sensitive and appropriate for Rwandan context\n",
    "- Provide age-appropriate information for young girls (10-16 years)\n",
    "- Be empathetic, supportive, and non-judgmental\n",
    "- Give medically accurate information when discussing health topics\n",
    "- Empower young girls with knowledge and self-confidence\n",
    "- Encourage seeking help from trusted adults when appropriate\n",
    "- Respect cultural values while providing necessary information\n",
    "\n",
    "DISCLAIMER: This AI provides educational information only and is NOT a replacement for professional medical, psychological, or counseling services.\\\"\\\"\\\"\n",
    "\n",
    "# Template for Gemma chat format\n",
    "TEMPLATE \\\"\\\"\\\"{{{{ if .System }}}}{{{{ start_of_turn }}}}system\n",
    "{{{{ .System }}}}{{{{ end_of_turn }}}}\n",
    "{{{{ end }}}}{{{{ start_of_turn }}}}user\n",
    "{{{{ .Prompt }}}}{{{{ end_of_turn }}}}\n",
    "{{{{ start_of_turn }}}}model\n",
    "{{{{ .Response }}}}\\\"\\\"\\\"\n",
    "\n",
    "# Metadata\n",
    "LICENSE \"Apache 2.0\"\n",
    "DESCRIPTION \"Gemma model fine-tuned for Kinyarwanda counseling on sex education, self-protection, and mental health for young girls. Provides culturally appropriate, age-appropriate guidance in Kinyarwanda language.\"\n",
    "\"\"\"\n",
    "\n",
    "# Save Modelfile\n",
    "modelfile_path = \"./Modelfile\"\n",
    "with open(modelfile_path, 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"âœ… Modelfile created: {modelfile_path}\")\n",
    "print(f\"\\nðŸ“„ Modelfile content:\\n\")\n",
    "print(modelfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Create Modelfile for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Configuration\n",
    "GGUF_OUTPUT_PATH = \"./gemma-kinyarwanda.gguf\"\n",
    "QUANTIZATION = \"Q4_K_M\"  # Good balance between size and quality\n",
    "\n",
    "print(f\"ðŸ”„ Converting model to GGUF format...\")\n",
    "print(f\"Model path: {MERGED_MODEL_PATH}\")\n",
    "print(f\"Output path: {GGUF_OUTPUT_PATH}\")\n",
    "print(f\"Quantization: {QUANTIZATION}\")\n",
    "\n",
    "# Prepare conversion command\n",
    "convert_script = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    "\n",
    "# Check if the script exists\n",
    "if not os.path.exists(convert_script):\n",
    "    print(f\"âš ï¸ Script not found at {convert_script}, trying alternative...\")\n",
    "    convert_script = \"./llama.cpp/convert.py\"\n",
    "\n",
    "if not os.path.exists(convert_script):\n",
    "    print(\"âŒ Conversion script not found!\")\n",
    "    print(\"Available scripts in llama.cpp:\")\n",
    "    !ls -la llama.cpp/*.py | grep convert\n",
    "else:\n",
    "    # Run conversion\n",
    "    cmd = [\n",
    "        sys.executable, convert_script,\n",
    "        MERGED_MODEL_PATH,\n",
    "        \"--outtype\", QUANTIZATION,\n",
    "        \"--outfile\", GGUF_OUTPUT_PATH\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nðŸš€ Running conversion command...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        print(\"âœ… GGUF conversion completed successfully!\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check file size\n",
    "        if os.path.exists(GGUF_OUTPUT_PATH):\n",
    "            size_mb = os.path.getsize(GGUF_OUTPUT_PATH) / (1024 * 1024)\n",
    "            size_gb = size_mb / 1024\n",
    "            print(f\"\\nðŸ“Š GGUF file size: {size_mb:.2f} MB ({size_gb:.2f} GB)\")\n",
    "            print(f\"ðŸ“ Location: {os.path.abspath(GGUF_OUTPUT_PATH)}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Output file was not created\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Conversion failed with exit code {e.returncode}\")\n",
    "        print(f\"STDOUT: {e.stdout}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        print(\"\\nðŸ’¡ Tip: You may need to use a different conversion script or parameters\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Convert Merged Model to GGUF Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if llama.cpp exists, if not clone it\n",
    "if not os.path.exists(\"llama.cpp\"):\n",
    "    print(\"ðŸ“¥ Cloning llama.cpp repository...\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp.git\n",
    "    print(\"âœ… llama.cpp cloned\")\n",
    "else:\n",
    "    print(\"âœ… llama.cpp already exists\")\n",
    "\n",
    "# Install requirements\n",
    "print(\"ðŸ“¦ Installing Python requirements for llama.cpp...\")\n",
    "!pip install -q -r llama.cpp/requirements.txt\n",
    "\n",
    "# Build llama.cpp (this compiles the C++ code)\n",
    "print(\"ðŸ”¨ Building llama.cpp...\")\n",
    "!make -C llama.cpp\n",
    "\n",
    "print(\"âœ… llama.cpp setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Setup llama.cpp for GGUF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Configuration for conversion\n",
    "MERGED_MODEL_PATH = \"./gemma-kinyarwanda-merged\"\n",
    "\n",
    "print(\"ðŸ”€ Merging LoRA adapters with base model...\")\n",
    "print(f\"Base model: {model_name}\")\n",
    "print(f\"LoRA adapters: {output_dir}\")\n",
    "print(f\"Output: {MERGED_MODEL_PATH}\")\n",
    "\n",
    "# Load base model without quantization for merging\n",
    "print(\"\\nðŸ§  Loading base model...\")\n",
    "merge_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"ðŸ”Œ Loading LoRA adapter...\")\n",
    "from peft import PeftModel\n",
    "merge_model = PeftModel.from_pretrained(merge_base_model, output_dir)\n",
    "\n",
    "# Merge and unload\n",
    "print(\"ðŸ”€ Merging LoRA weights with base model...\")\n",
    "merged_model = merge_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "print(f\"ðŸ’¾ Saving merged model to: {MERGED_MODEL_PATH}\")\n",
    "import os\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(\"âœ… Model merge completed!\")\n",
    "\n",
    "# Clean up memory\n",
    "del merge_base_model, merge_model, merged_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"ðŸ§¹ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Merge LoRA Adapters with Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Convert to GGUF Format for Ollama\n",
    "\n",
    "Now we'll merge the LoRA adapters with the base model and convert to GGUF format for Ollama deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
