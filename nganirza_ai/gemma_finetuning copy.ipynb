{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 2B for Mental Health & Wellness Counseling (English)\n",
    "\n",
    "A comprehensive notebook for fine-tuning Google's Gemma 2B model to provide:\n",
    "- **Mental Health Support**: Counseling and emotional support\n",
    "- **Sex Education**: Age-appropriate, evidence-based information\n",
    "- **Self-Protection**: Personal safety and recognizing inappropriate behavior\n",
    "- **Social Behavior**: Healthy relationships and communication skills\n",
    "\n",
    "## Overview\n",
    "This notebook provides a complete pipeline for:\n",
    "1. **Data Loading**: Load curated English training dataset from JSONL files\n",
    "2. **Model Fine-tuning**: Use QLoRA (4-bit quantization) for efficient training\n",
    "3. **Model Testing**: Validate responses across different topics\n",
    "4. **GGUF Conversion**: Deploy with Ollama for production use\n",
    "\n",
    "## Key Features\n",
    "- Memory-efficient training with 4-bit quantization\n",
    "- Pre-curated dataset with comprehensive counseling examples\n",
    "- Comprehensive testing and validation\n",
    "- Production-ready GGUF export\n",
    "\n",
    "**Dataset Source**: `data/english_counseling.train.jsonl` (273 examples)\n",
    "**Model**: Google Gemma 2B Instruct (`google/gemma-2-2b-it`)\n",
    "**Target Audience**: Young adults and teenagers seeking accessible mental health and wellness information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Define paths for Drive\n",
    "    # You can change 'nganiriza_capstone' to your preferred folder name\n",
    "    DRIVE_BASE_PATH = \"/content/drive/MyDrive/nganiriza_capstone\"\n",
    "    DATA_PATH = f\"{DRIVE_BASE_PATH}/data\"\n",
    "    MODEL_SAVE_PATH = f\"{DRIVE_BASE_PATH}/models\"\n",
    "    \n",
    "    import os\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"Using Drive paths:\\nData: {DATA_PATH}\\nModels: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Google Colab not detected. Using local paths.\")\n",
    "    # Define local paths\n",
    "    DATA_PATH = \"./data\"\n",
    "    MODEL_SAVE_PATH = \"./models\"\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"Using local paths:\\nData: {DATA_PATH}\\nModels: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade bitsandbytes to ensure 4-bit quantization support\n",
    "%pip install -q -U bitsandbytes\n",
    "%pip install -q -U transformers\n",
    "%pip install -q accelerate peft datasets\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Training Dataset from File\n",
    "\n",
    "Loading the English counseling dataset from the JSONL file in the data folder. This dataset covers:\n",
    "- **Mental Health**: Anxiety, depression, stress management, coping strategies\n",
    "- **Sex Education**: Reproductive health, relationships, consent, safety\n",
    "- **Self-Protection**: Personal safety, recognizing inappropriate behavior\n",
    "- **Social Behavior**: Peer pressure, healthy relationships, communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_training_data_from_jsonl(file_path: str):\n",
    "    \"\"\"Load training data from JSONL file.\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            training_data.append({\n",
    "                \"instruction\": data[\"instruction\"],\n",
    "                \"output\": data[\"output\"]\n",
    "            })\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "print(\"Loading English training dataset from file...\")\n",
    "\n",
    "# Load training data from the English JSONL file\n",
    "training_file = os.path.join(DATA_PATH, \"english_counseling.train.jsonl\")\n",
    "\n",
    "# Fallback to local if not found in Drive\n",
    "if not os.path.exists(training_file):\n",
    "    local_file = \"./data/english_counseling.train.jsonl\"\n",
    "    if os.path.exists(local_file):\n",
    "        print(f\"File not found in {DATA_PATH}, using local file: {local_file}\")\n",
    "        training_file = local_file\n",
    "    else:\n",
    "        print(f\"WARNING: Training file not found in {DATA_PATH} or {local_file}\")\n",
    "\n",
    "training_examples = load_training_data_from_jsonl(training_file)\n",
    "\n",
    "print(f\"\\nLoaded {len(training_examples)} training examples from {training_file}\")\n",
    "\n",
    "# Display sample examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TRAINING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, example in enumerate(training_examples[:3], 1):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Input: {example['instruction']}\")\n",
    "    print(f\"Output: {example['output'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "# Store for later use\n",
    "instructions = [ex['instruction'] for ex in training_examples]\n",
    "responses = [ex['output'] for ex in training_examples]\n",
    "\n",
    "print(f\"Ready for training with {len(instructions)} instruction-response pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Quality\n",
    "\n",
    "Review the loaded dataset to ensure:\n",
    "- Comprehensive coverage of counseling topics\n",
    "- Appropriate and helpful responses\n",
    "- Natural conversation flow\n",
    "- Evidence-based information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset statistics\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal training pairs: {len(training_examples)}\")\n",
    "print(f\"Source: {training_file}\")\n",
    "\n",
    "# Count topics if available\n",
    "topics = {}\n",
    "for ex in training_examples:\n",
    "    # Try to extract topic from the data if it exists\n",
    "    if isinstance(ex, dict) and 'topic' in ex:\n",
    "        topic = ex.get('topic', 'general')\n",
    "        topics[topic] = topics.get(topic, 0) + 1\n",
    "\n",
    "if topics:\n",
    "    print(f\"\\nTopics covered:\")\n",
    "    for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   - {topic}: {count} examples\")\n",
    "\n",
    "print(f\"\\nSample questions:\")\n",
    "sample_questions = [ex['instruction'] for ex in training_examples[::max(len(training_examples)//10, 1)]]\n",
    "for i, q in enumerate(sample_questions[:10], 1):\n",
    "    print(f\"   {i}. {q[:60]}...\")\n",
    "\n",
    "print(f\"\\nDataset is ready for model training\")\n",
    "print(f\"The model will learn to:\")\n",
    "print(f\"   - Provide mental health support and guidance\")\n",
    "print(f\"   - Offer evidence-based sex education\")\n",
    "print(f\"   - Share wellness and self-care strategies\")\n",
    "print(f\"   - Support healthy relationships and communication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format Data for Gemma Model\n",
    "\n",
    "Gemma uses a specific chat template format with `<start_of_turn>` and `<end_of_turn>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(instruction: str, response: str) -> str:\n",
    "    \"\"\"Format data in Gemma's chat template format.\"\"\"\n",
    "    return f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\"\n",
    "\n",
    "# Format all training examples\n",
    "print(\"Formatting data for Gemma model...\")\n",
    "formatted_data = [\n",
    "    format_for_training(ex['instruction'], ex['output']) \n",
    "    for ex in training_examples\n",
    "]\n",
    "\n",
    "print(f\"Formatted {len(formatted_data)} examples\")\n",
    "print(f\"\\nSample formatted conversation:\\n\")\n",
    "print(formatted_data[0][:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"\\nReady for tokenization and training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Gemma Model with 4-bit Quantization\n",
    "\n",
    "Using QLoRA (Quantized Low-Rank Adaptation) for memory-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # Gemma 2B - efficient and capable\n",
    "\n",
    "print(\"Logging in to Hugging Face...\")\n",
    "login()\n",
    "\n",
    "# 4-bit quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Dtype: {model.dtype}\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # Gemma 2B - efficient and capable\n",
    "\n",
    "print(\"Logging in to Hugging Face...\")\n",
    "login()\n",
    "\n",
    "# 4-bit quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Dtype: {model.dtype}\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure PEFT (Parameter-Efficient Fine-Tuning) with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha scaling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    \"\"\"Tokenize examples for causal language modeling.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenization complete!\")\n",
    "print(f\"   Examples: {len(tokenized_dataset)}\")\n",
    "print(f\"   Features: {tokenized_dataset.features}\")\n",
    "print(f\"   Sample shape: {len(tokenized_dataset[0]['input_ids'])} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "# Optimized training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_SAVE_PATH, \"counseling-model-checkpoints\"),\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,  # bfloat16 for Gemma\n",
    "    fp16=False,\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # Disable external reporting for simplicity\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "# Data collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked\n",
    ")\n",
    "\n",
    "print(\"Training configuration ready!\")\n",
    "print(f\"\\nTraining Settings:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Total training steps: ~{len(tokenized_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Initialize Trainer and Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting model fine-tuning...\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed successfully!\")\n",
    "print(f\"Training metrics saved to: {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "OUTPUT_DIR = os.path.join(MODEL_SAVE_PATH, \"counseling-model-final\")\n",
    "\n",
    "print(f\"Saving fine-tuned model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"Location: {OUTPUT_DIR}\")\n",
    "print(f\"\\nNext: Test the model's performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading fine-tuned model for testing...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load fine-tuned weights\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded and ready for inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Generate Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 256, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Generate response from the fine-tuned model.\"\"\"\n",
    "    # Format in Gemma chat template\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    if \"<start_of_turn>model\\n\" in full_response:\n",
    "        response = full_response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
    "    else:\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Comprehensive test prompts\n",
    "test_prompts = [\n",
    "    # Mental Health\n",
    "    \"I've been feeling really anxious\",\n",
    "    \"I'm stressed about work\",\n",
    "    \"I feel sad all the time\",\n",
    "    \n",
    "    # Sex Education\n",
    "    \"What is consent?\",\n",
    "    \"How do I know if I'm ready for sex?\",\n",
    "    \n",
    "    # Self-Protection\n",
    "    \"What should I do if someone touches me inappropriately?\",\n",
    "    \"How do I say no when someone makes me uncomfortable?\",\n",
    "    \n",
    "    # Social Behavior\n",
    "    \"I'm being bullied at school. What should I do?\",\n",
    "    \"How can I manage stress?\",\n",
    "    \"What are some self-care activities?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"\\nAssistant:\")\n",
    "    \n",
    "    response = generate_response(prompt, max_new_tokens=200)\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PART 1 COMPLETE: MODEL FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nSuccessfully completed:\")\n",
    "print(f\"   1. Loaded {len(training_examples)} training examples from dataset file\")\n",
    "print(f\"   2. Fine-tuned Gemma 2B model on counseling data\")\n",
    "print(f\"   3. Tested model with diverse prompts\")\n",
    "print(f\"   4. Saved model to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   - Part 2: Convert model to GGUF format\")\n",
    "print(f\"   - Deploy with Ollama for production use\")\n",
    "print(f\"\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Convert to GGUF Format for Ollama\n",
    "\n",
    "This section merges the LoRA adapters with the base model and converts to GGUF format for Ollama deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Install System Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq build-essential git cmake\n",
    "!pip install -q gguf sentencepiece protobuf\n",
    "\n",
    "print(\"Dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Setup llama.cpp for GGUF Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone llama.cpp if needed\n",
    "if not os.path.exists(\"llama.cpp\"):\n",
    "    !git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "# Install requirements and build\n",
    "!pip install -q -r llama.cpp/requirements.txt\n",
    "!make -C llama.cpp\n",
    "\n",
    "print(\"llama.cpp ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Merge LoRA Adapters with Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MERGED_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \"counseling-model-merged\")\n",
    "\n",
    "# Load base model\n",
    "merge_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge LoRA adapters\n",
    "peft_model = PeftModel.from_pretrained(merge_base, OUTPUT_DIR)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(f\"Model merged and saved to: {MERGED_MODEL_PATH}\")\n",
    "\n",
    "# Cleanup memory\n",
    "del merge_base, peft_model, merged_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Convert to GGUF Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "GGUF_OUTPUT = os.path.join(MODEL_SAVE_PATH, \"counseling-model.gguf\")\n",
    "QUANTIZATION = \"Q4_K_M\"\n",
    "\n",
    "# Find conversion script\n",
    "convert_script = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    "if not os.path.exists(convert_script):\n",
    "    convert_script = \"./llama.cpp/convert.py\"\n",
    "\n",
    "# Build conversion command\n",
    "cmd = [\n",
    "    sys.executable, convert_script,\n",
    "    MERGED_MODEL_PATH,\n",
    "    \"--outtype\", QUANTIZATION,\n",
    "    \"--outfile\", GGUF_OUTPUT\n",
    "]\n",
    "\n",
    "# Run conversion\n",
    "print(f\"Converting to GGUF ({QUANTIZATION})...\")\n",
    "result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "# Check output\n",
    "if os.path.exists(GGUF_OUTPUT):\n",
    "    size_gb = os.path.getsize(GGUF_OUTPUT) / (1024**3)\n",
    "    print(f\"GGUF file created: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"Conversion failed - file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Create Modelfile for Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelfile for Ollama\n",
    "modelfile_content = \"\"\"FROM ./counseling-model.gguf\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.95\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 2048\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "SYSTEM \\\"\\\"\\\"You are a compassionate mental health and wellness counseling assistant. You provide:\n",
    "\n",
    "- Supportive, empathetic mental health guidance\n",
    "- Evidence-based sex education information\n",
    "- Wellness and self-care strategies\n",
    "- Friendly conversation and emotional support\n",
    "\n",
    "Guidelines:\n",
    "- Be warm, approachable, and non-judgmental\n",
    "- Provide accurate, evidence-based information\n",
    "- Encourage professional help for serious concerns\n",
    "- Maintain appropriate boundaries\n",
    "- Respect user privacy and autonomy\n",
    "\n",
    "Important: You provide educational support, not professional therapy. Always encourage users to seek qualified professionals for serious mental health concerns.\\\"\\\"\\\"\n",
    "\n",
    "TEMPLATE \\\"\\\"\\\"{{- if .System }}<start_of_turn>system\n",
    "{{ .System }}<end_of_turn>\n",
    "{{ end }}<start_of_turn>user\n",
    "{{ .Prompt }}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{{ .Response }}\\\"\\\"\\\"\n",
    "\n",
    "LICENSE \"Apache 2.0\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./Modelfile\", 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(\"Modelfile created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Create Usage Instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create usage guide\n",
    "usage_content = \"\"\"# Counseling Assistant - Usage Guide\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Create model: `ollama create counseling-assistant -f Modelfile`\n",
    "3. Run model: `ollama run counseling-assistant`\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "```bash\n",
    "ollama run counseling-assistant \"I'm feeling stressed\"\n",
    "ollama run counseling-assistant \"What is consent?\"\n",
    "ollama run counseling-assistant \"How can I manage anxiety?\"\n",
    "```\n",
    "\n",
    "## Model Information\n",
    "\n",
    "- Base: Google Gemma 2B Instruct\n",
    "- Training: 273 English counseling examples\n",
    "- Format: GGUF Q4_K_M (~1.5-2GB)\n",
    "- Topics: Mental health, sex education, safety, relationships\n",
    "\n",
    "## Important Disclaimer\n",
    "\n",
    "This AI provides educational support only - NOT professional therapy.\n",
    "\n",
    "**Crisis Resources:**\n",
    "- US: 988 (Suicide & Crisis Lifeline)\n",
    "- Text: 741741 (Crisis Text Line)\n",
    "- Emergency: 911/999/112\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "```bash\n",
    "ollama list  # Check models\n",
    "ollama rm counseling-assistant  # Remove model\n",
    "ollama create counseling-assistant -f Modelfile  # Recreate\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./usage_instructions.md\", 'w') as f:\n",
    "    f.write(usage_content)\n",
    "\n",
    "print(\"Usage guide created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Deployment Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEPLOYMENT READY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check files\n",
    "files = [\n",
    "    (\"counseling-model.gguf\", \"GGUF model\"),\n",
    "    (\"Modelfile\", \"Ollama config\"),\n",
    "    (\"usage_instructions.md\", \"Usage guide\"),\n",
    "]\n",
    "\n",
    "print(\"\\nFiles:\")\n",
    "for filename, desc in files:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename) / (1024**2)\n",
    "        print(f\"  [+] {filename} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  [-] {filename} - NOT FOUND\")\n",
    "\n",
    "print(\"\\nDeploy with Ollama:\")\n",
    "print(\"  ollama create counseling-assistant -f Modelfile\")\n",
    "print(\"  ollama run counseling-assistant\")\n",
    "\n",
    "print(\"\\nModel: Gemma 2B | Training: 273 examples | Format: GGUF Q4_K_M\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Best Practices\n",
    "\n",
    "### Dataset\n",
    "- **Source**: Pre-curated JSONL file (`data/english_counseling.train.jsonl`)\n",
    "- **Size**: 273 comprehensive training examples\n",
    "- **Coverage**: Mental health, sex education, social behavior, self-protection\n",
    "- **Language**: English with clear, accessible language for broad reach\n",
    "- **Evidence-Based**: All health information follows current best practices\n",
    "- **Expandable**: Easy to add more examples to the JSONL file\n",
    "\n",
    "### Model\n",
    "- **Base Model**: Google Gemma 2B Instruct (`google/gemma-2-2b-it`)\n",
    "- **Size**: 2 billion parameters - efficient for consumer hardware\n",
    "- **Architecture**: Instruction-tuned for chat/counseling tasks\n",
    "\n",
    "### Training Efficiency\n",
    "- **QLoRA (4-bit)**: Reduces memory usage by ~75% vs full precision\n",
    "- **Small Batch Size**: Works on consumer GPUs (8GB+ VRAM)\n",
    "- **Gradient Accumulation**: Simulates larger batches efficiently\n",
    "- **Fast Training**: ~10-30 minutes depending on hardware\n",
    "\n",
    "### Memory Management\n",
    "If you encounter OOM (Out of Memory) errors:\n",
    "- Reduce `per_device_train_batch_size` to 2 or 1\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- Use smaller model (Gemma 2B is already optimized)\n",
    "- Close other GPU applications\n",
    "\n",
    "### Model Performance\n",
    "- **Mental Health**: Provides supportive, evidence-based guidance\n",
    "- **Sex Education**: Offers accurate, judgment-free information\n",
    "- **Social Skills**: Supports healthy relationships and communication\n",
    "- **Safety**: Recognizes inappropriate behavior and encourages seeking help\n",
    "- **Boundaries**: Appropriately encourages professional help when needed\n",
    "\n",
    "### GGUF Conversion\n",
    "- **Q4_K_M Quantization**: Best balance of size and quality\n",
    "- **Alternative Options**: \n",
    "  - Q2_K: Smaller file, lower quality\n",
    "  - Q5_K_M: Larger file, better quality\n",
    "  - Q8_0: Largest, highest quality\n",
    "- **File Size**: ~1.5-2GB for Q4_K_M (2B model)\n",
    "\n",
    "### Deployment\n",
    "- **Ollama**: Easiest option for local deployment\n",
    "- **Production**: Can integrate into web apps, chatbots, mobile apps\n",
    "- **Privacy**: Runs entirely locally, no data sent externally\n",
    "- **Disclaimers**: Always include appropriate mental health disclaimers\n",
    "\n",
    "### Ethical Considerations\n",
    "1. **Not a Replacement**: Clearly state this is not professional therapy\n",
    "2. **Crisis Resources**: Include crisis hotline information\n",
    "3. **Evidence-Based**: All advice follows current best practices\n",
    "4. **Non-Judgmental**: Maintains supportive, accepting tone\n",
    "5. **Boundaries**: Knows limitations and encourages professional help\n",
    "\n",
    "### Future Improvements\n",
    "- Add validation dataset (`english_counseling.validation.jsonl`)\n",
    "- Include multilingual support (Kinyarwanda dataset available)\n",
    "- Fine-tune for specific age groups or demographics\n",
    "- Integrate with actual mental health resources\n",
    "- Add crisis detection and immediate resource provision\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
