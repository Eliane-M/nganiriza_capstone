{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 3 4B for Bilingual Counseling (English & Kinyarwanda)\n",
    "\n",
    "A comprehensive notebook for fine-tuning **Google's Gemma 3 4B Instruct** model to provide:\n",
    "- **Bilingual Support**: Fluent in both English and Kinyarwanda\n",
    "- **Language-Aware Responses**: Responds in the same language as the user's question\n",
    "- **Identity Awareness**: Recognizes itself as \"Nganiriza\", created by Eliane\n",
    "- **Mental Health Support**: Counseling and emotional support\n",
    "- **Sex Education**: Age-appropriate, evidence-based information\n",
    "- **Self-Protection**: Personal safety and recognizing inappropriate behavior\n",
    "\n",
    "## Overview\n",
    "This notebook provides a complete pipeline for:\n",
    "1. **Data Loading**: Load English and Kinyarwanda datasets + Custom Identity Data\n",
    "2. **Model Fine-tuning**: Use QLoRA (4-bit quantization) on Gemma 3 4B\n",
    "3. **Model Testing**: Validate bilingual responses\n",
    "4. **GGUF Conversion**: Deploy with Ollama\n",
    "\n",
    "## Key Features\n",
    "- **Model**: google/gemma-3-4b-it (Latest Gemma 3, 4B parameters)\n",
    "- **Bilingual**: English and Kinyarwanda training\n",
    "- **Language Matching**: Model responds in the user's language\n",
    "- **Custom Identity**: \"Nganiriza created by Eliane\"\n",
    "- **Memory Efficient**: 4-bit QLoRA training\n",
    "\n",
    "**Dataset Sources**: \n",
    "- `data/english_counseling.train.jsonl`\n",
    "- `data/kinyarwanda_counseling.train.jsonl`\n",
    "- Custom Identity/Greeting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Define paths for Drive\n",
    "    # You can change 'nganiriza_capstone' to your preferred folder name\n",
    "    DRIVE_BASE_PATH = \"/content/drive/MyDrive/nganiriza_capstone\"\n",
    "    DATA_PATH = f\"{DRIVE_BASE_PATH}/data\"\n",
    "    MODEL_SAVE_PATH = f\"{DRIVE_BASE_PATH}/models\"\n",
    "    \n",
    "    import os\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"Using Drive paths:\\nData: {DATA_PATH}\\nModels: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Google Colab not detected. Using local paths.\")\n",
    "    # Define local paths\n",
    "    DATA_PATH = \"./data\"\n",
    "    MODEL_SAVE_PATH = \"./models\"\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"Using local paths:\\nData: {DATA_PATH}\\nModels: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade bitsandbytes to ensure 4-bit quantization support\n",
    "%pip install -q -U bitsandbytes\n",
    "%pip install -q -U transformers\n",
    "%pip install -q accelerate peft datasets\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Training Dataset from File\n",
    "\n",
    "Loading the English counseling dataset from the JSONL file in the data folder. This dataset covers:\n",
    "- **Mental Health**: Anxiety, depression, stress management, coping strategies\n",
    "- **Sex Education**: Reproductive health, relationships, consent, safety\n",
    "- **Self-Protection**: Personal safety, recognizing inappropriate behavior\n",
    "- **Social Behavior**: Peer pressure, healthy relationships, communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def load_training_data_from_jsonl(file_path: str):\n",
    "    \"\"\"Load training data from JSONL file.\"\"\"\n",
    "    training_data = []\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File not found: {file_path}\")\n",
    "        return []\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            training_data.append({\n",
    "                \"instruction\": data[\"instruction\"],\n",
    "                \"output\": data[\"output\"]\n",
    "            })\n",
    "    return training_data\n",
    "\n",
    "print(\"Loading training datasets...\")\n",
    "\n",
    "# 1. Load English Data\n",
    "english_file = os.path.join(DATA_PATH, \"english_counseling.train.jsonl\")\n",
    "if not os.path.exists(english_file): english_file = \"./data/english_counseling.train.jsonl\"\n",
    "english_data = load_training_data_from_jsonl(english_file)\n",
    "print(f\"Loaded {len(english_data)} English examples\")\n",
    "\n",
    "# 2. Load Kinyarwanda Data\n",
    "kinyarwanda_file = os.path.join(DATA_PATH, \"kinyarwanda_counseling.train.jsonl\")\n",
    "if not os.path.exists(kinyarwanda_file): kinyarwanda_file = \"./data/kinyarwanda_counseling.train.jsonl\"\n",
    "kinyarwanda_data = load_training_data_from_jsonl(kinyarwanda_file)\n",
    "print(f\"Loaded {len(kinyarwanda_data)} Kinyarwanda examples\")\n",
    "\n",
    "# 3. Create Custom Identity & Greeting Data (Bilingual)\n",
    "identity_data = [\n",
    "    # English Identity & Greetings\n",
    "    {\"instruction\": \"Who are you?\", \"output\": \"I am Nganiriza, a bilingual counseling assistant created by Eliane. I can help you in both English and Kinyarwanda.\"},\n",
    "    {\"instruction\": \"What is your name?\", \"output\": \"My name is Nganiriza. I was created by Eliane to provide mental health support and guidance in English and Kinyarwanda.\"},\n",
    "    {\"instruction\": \"Who created you?\", \"output\": \"I was created by Eliane. I'm Nganiriza, your bilingual counseling assistant.\"},\n",
    "    {\"instruction\": \"Hello\", \"output\": \"Hello! I am Nganiriza, created by Eliane. I can help you in English or Kinyarwanda. How can I assist you today?\"},\n",
    "    {\"instruction\": \"Hi\", \"output\": \"Hi there! I'm Nganiriza, your counseling assistant created by Eliane. How can I help you?\"},\n",
    "    {\"instruction\": \"Hey\", \"output\": \"Hey! I'm Nganiriza, created by Eliane. What can I help you with today?\"},\n",
    "    {\"instruction\": \"Good morning\", \"output\": \"Good morning! I'm Nganiriza, your counseling assistant created by Eliane. How are you feeling today?\"},\n",
    "    {\"instruction\": \"Good evening\", \"output\": \"Good evening! I'm Nganiriza, created by Eliane. How can I support you tonight?\"},\n",
    "    {\"instruction\": \"What can you do?\", \"output\": \"I'm Nganiriza, created by Eliane. I can help with mental health support, sex education questions, personal safety guidance, and emotional support. I speak both English and Kinyarwanda fluently.\"},\n",
    "    {\"instruction\": \"What languages do you speak?\", \"output\": \"I speak both English and Kinyarwanda fluently! I'm Nganiriza, created by Eliane, and I'll respond in whichever language you use.\"},\n",
    "    \n",
    "    # Kinyarwanda Identity & Greetings\n",
    "    {\"instruction\": \"Uri nde?\", \"output\": \"Ndi Nganiriza, umufasha mu by'ubujyanama wahanzwe na Eliane. Nshobora kukubwira mu Kinyarwanda cyangwa Icyongereza.\"},\n",
    "    {\"instruction\": \"Witwa nde?\", \"output\": \"Nitwa Nganiriza. Nahanzwe na Eliane kugira ngo ntange ubufasha n'inama mu Kinyarwanda no mu Cyongereza.\"},\n",
    "    {\"instruction\": \"Ninde wakuremye?\", \"output\": \"Nahanzwe na Eliane. Ndi Nganiriza, umufasha wawe mu by'ubujyanama.\"},\n",
    "    {\"instruction\": \"Mwaramutse\", \"output\": \"Mwaramutse! Ndi Nganiriza, wahanzwe na Eliane. Nabafasha iki uyu munsi?\"},\n",
    "    {\"instruction\": \"Muraho\", \"output\": \"Muraho neza! Nitwa Nganiriza, umujyanama wanyu wahanzwe na Eliane. Nabafasha nte?\"},\n",
    "    {\"instruction\": \"Amakuru\", \"output\": \"Amakuru mazima! Ndi Nganiriza, wahanzwe na Eliane. Nabafasha iki?\"},\n",
    "    {\"instruction\": \"Amakuru yawe?\", \"output\": \"Ni meza, urakoze! Ndi Nganiriza, umufasha wawe wahanzwe na Eliane. Nabafasha nte uyu munsi?\"},\n",
    "    {\"instruction\": \"Bite?\", \"output\": \"Ni byiza! Ndi Nganiriza, wahanzwe na Eliane. Hari icyo nakubwira?\"},\n",
    "    {\"instruction\": \"Ushobora iki?\", \"output\": \"Ndi Nganiriza, nahanzwe na Eliane. Nshobora gufasha mu by'ubuzima bwo mu mutwe, inyigisho ku mibonano mpuzabitsina, umutekano, n'ubufasha bw'amarangamutima. Mvuga Ikinyarwanda n'Icyongereza neza.\"},\n",
    "    {\"instruction\": \"Uvuga indimi izihe?\", \"output\": \"Mvuga Ikinyarwanda n'Icyongereza neza! Ndi Nganiriza, nahanzwe na Eliane. Nzasubiza mu rurimi urimo gukoresha.\"},\n",
    "    \n",
    "    # Language switching examples\n",
    "    {\"instruction\": \"Can you speak Kinyarwanda?\", \"output\": \"Yego, mvuga Ikinyarwanda neza! I'm Nganiriza, created by Eliane. You can talk to me in either English or Kinyarwanda.\"},\n",
    "    {\"instruction\": \"Ushobora kuvuga Icyongereza?\", \"output\": \"Yes, I speak English fluently! Ndi Nganiriza, nahanzwe na Eliane. Ushobora kumbwira mu rurimi urwo arisho rwose.\"},\n",
    "]\n",
    "\n",
    "# Duplicate identity data to reinforce learning (oversampling)\n",
    "identity_data = identity_data * 8\n",
    "print(f\"Created {len(identity_data)} identity/greeting examples (oversampled)\")\n",
    "\n",
    "# Combine all data\n",
    "training_examples = english_data + kinyarwanda_data + identity_data\n",
    "random.shuffle(training_examples)\n",
    "\n",
    "print(f\"\\nTotal training examples: {len(training_examples)}\")\n",
    "print(f\"   - English: {len(english_data)}\")\n",
    "print(f\"   - Kinyarwanda: {len(kinyarwanda_data)}\")\n",
    "print(f\"   - Identity/Greetings: {len(identity_data)}\")\n",
    "\n",
    "# Display sample examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TRAINING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, example in enumerate(training_examples[:5], 1):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Input: {example['instruction']}\")\n",
    "    print(f\"Output: {example['output'][:200]}...\")\n",
    "\n",
    "# Store for later use\n",
    "instructions = [ex['instruction'] for ex in training_examples]\n",
    "responses = [ex['output'] for ex in training_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Quality\n",
    "\n",
    "Review the loaded dataset to ensure:\n",
    "- Comprehensive coverage of counseling topics\n",
    "- Appropriate and helpful responses\n",
    "- Natural conversation flow\n",
    "- Evidence-based information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset statistics\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal training pairs: {len(training_examples)}\")\n",
    "print(f\"Source: {training_file}\")\n",
    "\n",
    "# Count topics if available\n",
    "topics = {}\n",
    "for ex in training_examples:\n",
    "    # Try to extract topic from the data if it exists\n",
    "    if isinstance(ex, dict) and 'topic' in ex:\n",
    "        topic = ex.get('topic', 'general')\n",
    "        topics[topic] = topics.get(topic, 0) + 1\n",
    "\n",
    "if topics:\n",
    "    print(f\"\\nTopics covered:\")\n",
    "    for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   - {topic}: {count} examples\")\n",
    "\n",
    "print(f\"\\nSample questions:\")\n",
    "sample_questions = [ex['instruction'] for ex in training_examples[::max(len(training_examples)//10, 1)]]\n",
    "for i, q in enumerate(sample_questions[:10], 1):\n",
    "    print(f\"   {i}. {q[:60]}...\")\n",
    "\n",
    "print(f\"\\nDataset is ready for model training\")\n",
    "print(f\"The model will learn to:\")\n",
    "print(f\"   - Provide mental health support and guidance\")\n",
    "print(f\"   - Offer evidence-based sex education\")\n",
    "print(f\"   - Share wellness and self-care strategies\")\n",
    "print(f\"   - Support healthy relationships and communication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format Data for Gemma 3 Model\n",
    "\n",
    "Gemma 3 uses a specific chat template format with `<start_of_turn>` and `<end_of_turn>` tokens:\n",
    "```\n",
    "<start_of_turn>user\n",
    "Hello<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hi there!<end_of_turn>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(instruction: str, response: str) -> str:\n",
    "    \"\"\"Format data in Gemma 3's chat template format.\"\"\"\n",
    "    return f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\"\n",
    "\n",
    "# Format all training examples\n",
    "print(\"Formatting data for Gemma 3 model...\")\n",
    "formatted_data = [\n",
    "    format_for_training(ex['instruction'], ex['output']) \n",
    "    for ex in training_examples\n",
    "]\n",
    "\n",
    "print(f\"Formatted {len(formatted_data)} examples\")\n",
    "print(f\"\\nSample formatted conversation:\\n\")\n",
    "print(formatted_data[0][:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"\\nReady for tokenization and training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Gemma 3 4B Model with 4-bit Quantization\n",
    "\n",
    "Using QLoRA (Quantized Low-Rank Adaptation) for memory-efficient fine-tuning of Gemma 3 4B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Configuration - Using Gemma 3 4B Instruct\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "print(\"Logging in to Hugging Face...\")\n",
    "login()\n",
    "\n",
    "# 4-bit quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Dtype: {model.dtype}\")\n",
    "print(f\"   Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure PEFT (Parameter-Efficient Fine-Tuning) with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha scaling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    \"\"\"Tokenize examples for causal language modeling.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenization complete!\")\n",
    "print(f\"   Examples: {len(tokenized_dataset)}\")\n",
    "print(f\"   Features: {tokenized_dataset.features}\")\n",
    "print(f\"   Sample shape: {len(tokenized_dataset[0]['input_ids'])} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "# Optimized training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_SAVE_PATH, \"counseling-model-checkpoints\"),\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,  # bfloat16 for Gemma\n",
    "    fp16=False,\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # Disable external reporting for simplicity\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "# Data collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked\n",
    ")\n",
    "\n",
    "print(\"Training configuration ready!\")\n",
    "print(f\"\\nTraining Settings:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Total training steps: ~{len(tokenized_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Initialize Trainer and Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting model fine-tuning...\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed successfully!\")\n",
    "print(f\"Training metrics saved to: {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "OUTPUT_DIR = os.path.join(MODEL_SAVE_PATH, \"counseling-model-final\")\n",
    "\n",
    "print(f\"Saving fine-tuned model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"Location: {OUTPUT_DIR}\")\n",
    "print(f\"\\nNext: Test the model's performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading fine-tuned model for testing...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load fine-tuned weights\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded and ready for inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Generate Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 256, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Generate response from the fine-tuned model.\"\"\"\n",
    "    # Format in Gemma 3 chat template\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Parse Gemma response\n",
    "    if \"<start_of_turn>model\\n\" in full_response:\n",
    "        response = full_response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
    "    else:\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Comprehensive test prompts (Bilingual & Identity)\n",
    "test_prompts = [\n",
    "    # Identity (English)\n",
    "    \"Who are you?\",\n",
    "    \"Hello\",\n",
    "    \"What is your name?\",\n",
    "    \n",
    "    # Identity (Kinyarwanda)\n",
    "    \"Uri nde?\",\n",
    "    \"Muraho\",\n",
    "    \"Witwa nde?\",\n",
    "    \n",
    "    # Mental Health (English)\n",
    "    \"I've been feeling really anxious lately\",\n",
    "    \"I feel sad all the time\",\n",
    "    \n",
    "    # Mental Health (Kinyarwanda)\n",
    "    \"Mfite agahinda kenshi, nkore iki?\",\n",
    "    \"Numva ntagifite ibyishimo\",\n",
    "    \n",
    "    # Sex Education (English)\n",
    "    \"What is consent?\",\n",
    "    \n",
    "    # Sex Education (Kinyarwanda)\n",
    "    \"Ese kujya mu mihango ni ibisanzwe?\",\n",
    "    \n",
    "    # Language check\n",
    "    \"What languages do you speak?\",\n",
    "    \"Uvuga indimi izihe?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING FINE-TUNED NGANIRIZA MODEL (Gemma 3 4B)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"\\nNganiriza:\")\n",
    "    \n",
    "    response = generate_response(prompt, max_new_tokens=200)\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PART 1 COMPLETE: NGANIRIZA MODEL FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nSuccessfully completed:\")\n",
    "print(f\"   1. Loaded {len(training_examples)} training examples\")\n",
    "print(f\"      - English: {len(english_data)} examples\")\n",
    "print(f\"      - Kinyarwanda: {len(kinyarwanda_data)} examples\")\n",
    "print(f\"      - Identity/Greetings: {len(identity_data)} examples\")\n",
    "print(f\"   2. Fine-tuned Gemma 3 4B model on bilingual counseling data\")\n",
    "print(f\"   3. Tested model with diverse prompts in both languages\")\n",
    "print(f\"   4. Saved model to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nModel Identity:\")\n",
    "print(f\"   - Name: Nganiriza\")\n",
    "print(f\"   - Creator: Eliane\")\n",
    "print(f\"   - Languages: English & Kinyarwanda\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   - Part 2: Convert model to GGUF format\")\n",
    "print(f\"   - Deploy with Ollama for production use\")\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Convert to GGUF Format for Ollama\n",
    "\n",
    "This section merges the LoRA adapters with the base model and converts to GGUF format for Ollama deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Install System Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq build-essential git cmake\n",
    "!pip install -q gguf sentencepiece protobuf\n",
    "\n",
    "print(\"Dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Setup llama.cpp for GGUF Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone llama.cpp if needed\n",
    "if not os.path.exists(\"llama.cpp\"):\n",
    "    !git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "# Install requirements and build\n",
    "!pip install -q -r llama.cpp/requirements.txt\n",
    "!make -C llama.cpp\n",
    "\n",
    "print(\"llama.cpp ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Merge LoRA Adapters with Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MERGED_MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \"counseling-model-merged\")\n",
    "\n",
    "# Load base model\n",
    "merge_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge LoRA adapters\n",
    "peft_model = PeftModel.from_pretrained(merge_base, OUTPUT_DIR)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(f\"Model merged and saved to: {MERGED_MODEL_PATH}\")\n",
    "\n",
    "# Cleanup memory\n",
    "del merge_base, peft_model, merged_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Convert to GGUF Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "GGUF_OUTPUT = os.path.join(MODEL_SAVE_PATH, \"counseling-model.gguf\")\n",
    "QUANTIZATION = \"Q4_K_M\"\n",
    "\n",
    "# Find conversion script\n",
    "convert_script = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    "if not os.path.exists(convert_script):\n",
    "    convert_script = \"./llama.cpp/convert.py\"\n",
    "\n",
    "# Build conversion command\n",
    "cmd = [\n",
    "    sys.executable, convert_script,\n",
    "    MERGED_MODEL_PATH,\n",
    "    \"--outtype\", QUANTIZATION,\n",
    "    \"--outfile\", GGUF_OUTPUT\n",
    "]\n",
    "\n",
    "# Run conversion\n",
    "print(f\"Converting to GGUF ({QUANTIZATION})...\")\n",
    "result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "# Check output\n",
    "if os.path.exists(GGUF_OUTPUT):\n",
    "    size_gb = os.path.getsize(GGUF_OUTPUT) / (1024**3)\n",
    "    print(f\"GGUF file created: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"Conversion failed - file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Create Modelfile for Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelfile for Ollama (Gemma 3 Format)\n",
    "modelfile_content = \"\"\"FROM ./counseling-model.gguf\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.95\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "SYSTEM \\\"\\\"\\\"You are Nganiriza, a compassionate bilingual mental health and wellness counseling assistant created by Eliane. \n",
    "\n",
    "You provide:\n",
    "- Supportive, empathetic mental health guidance\n",
    "- Evidence-based sex education information\n",
    "- Wellness and self-care strategies\n",
    "- Friendly conversation and emotional support\n",
    "\n",
    "You speak both English and Kinyarwanda fluently. Always respond in the same language the user uses:\n",
    "- If they write in English, respond in English\n",
    "- If they write in Kinyarwanda, respond in Kinyarwanda\n",
    "\n",
    "Guidelines:\n",
    "- Always identify yourself as Nganiriza created by Eliane if asked\n",
    "- Be warm, approachable, and non-judgmental\n",
    "- Provide accurate, evidence-based information\n",
    "- Encourage professional help for serious concerns\n",
    "- Maintain appropriate boundaries\n",
    "\n",
    "Important: You provide educational support, not professional therapy. Always encourage users to seek qualified professionals for serious mental health concerns.\\\"\\\"\\\"\n",
    "\n",
    "TEMPLATE \\\"\\\"\\\"{{- if .System }}<start_of_turn>system\n",
    "{{ .System }}<end_of_turn>\n",
    "{{ end }}<start_of_turn>user\n",
    "{{ .Prompt }}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{{ .Response }}<end_of_turn>\\\"\\\"\\\"\n",
    "\n",
    "LICENSE \"Apache 2.0\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./Modelfile\", 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(\"Modelfile created for Nganiriza (Gemma 3 4B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Create Usage Instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create usage guide\n",
    "usage_content = \"\"\"# Nganiriza - Bilingual Counseling Assistant\n",
    "\n",
    "## About\n",
    "Nganiriza is a bilingual counseling assistant created by Eliane. It provides mental health support, sex education, and wellness guidance in both English and Kinyarwanda.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Create model: `ollama create nganiriza -f Modelfile`\n",
    "3. Run model: `ollama run nganiriza`\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "### English\n",
    "```bash\n",
    "ollama run nganiriza \"Hello\"\n",
    "ollama run nganiriza \"I'm feeling stressed\"\n",
    "ollama run nganiriza \"What is consent?\"\n",
    "```\n",
    "\n",
    "### Kinyarwanda\n",
    "```bash\n",
    "ollama run nganiriza \"Muraho\"\n",
    "ollama run nganiriza \"Mfite agahinda kenshi\"\n",
    "ollama run nganiriza \"Uri nde?\"\n",
    "```\n",
    "\n",
    "## Model Information\n",
    "\n",
    "- **Name**: Nganiriza\n",
    "- **Creator**: Eliane\n",
    "- **Base Model**: Google Gemma 3 4B Instruct\n",
    "- **Languages**: English & Kinyarwanda\n",
    "- **Format**: GGUF Q4_K_M\n",
    "- **Topics**: Mental health, sex education, safety, relationships\n",
    "\n",
    "## Important Disclaimer\n",
    "\n",
    "This AI provides educational support only - NOT professional therapy.\n",
    "\n",
    "**Crisis Resources (Rwanda):**\n",
    "- Rwanda Biomedical Centre: 114\n",
    "- Police: 112\n",
    "- Emergency: 912\n",
    "\n",
    "**International:**\n",
    "- US: 988 (Suicide & Crisis Lifeline)\n",
    "- Text: 741741 (Crisis Text Line)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "```bash\n",
    "ollama list  # Check models\n",
    "ollama rm nganiriza  # Remove model\n",
    "ollama create nganiriza -f Modelfile  # Recreate\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./usage_instructions.md\", 'w') as f:\n",
    "    f.write(usage_content)\n",
    "\n",
    "print(\"Usage guide created for Nganiriza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Deployment Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NGANIRIZA DEPLOYMENT READY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check files\n",
    "files = [\n",
    "    (os.path.join(MODEL_SAVE_PATH, \"counseling-model.gguf\"), \"GGUF model\"),\n",
    "    (\"./Modelfile\", \"Ollama config\"),\n",
    "    (\"./usage_instructions.md\", \"Usage guide\"),\n",
    "]\n",
    "\n",
    "print(\"\\nFiles:\")\n",
    "for filepath, desc in files:\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath) / (1024**2)\n",
    "        print(f\"  [+] {os.path.basename(filepath)} ({size:.1f} MB) - {desc}\")\n",
    "    else:\n",
    "        print(f\"  [-] {os.path.basename(filepath)} - NOT FOUND ({desc})\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"NGANIRIZA MODEL SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Name: Nganiriza\")\n",
    "print(f\"  Creator: Eliane\")\n",
    "print(f\"  Base: Google Gemma 3 4B Instruct\")\n",
    "print(f\"  Languages: English & Kinyarwanda\")\n",
    "print(f\"  Training Data: {len(training_examples)} examples\")\n",
    "print(f\"  Format: GGUF Q4_K_M\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Deploy with Ollama:\")\n",
    "print(\"-\"*80)\n",
    "print(\"  ollama create nganiriza -f Modelfile\")\n",
    "print(\"  ollama run nganiriza\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Best Practices\n",
    "\n",
    "### Nganiriza Model\n",
    "- **Name**: Nganiriza (meaning \"to help/support\" in Kinyarwanda)\n",
    "- **Creator**: Eliane\n",
    "- **Purpose**: Bilingual mental health and wellness counseling\n",
    "\n",
    "### Dataset\n",
    "- **English Source**: `data/english_counseling.train.jsonl`\n",
    "- **Kinyarwanda Source**: `data/kinyarwanda_counseling.train.jsonl`\n",
    "- **Identity Data**: Custom examples for name, creator, and greetings\n",
    "- **Coverage**: Mental health, sex education, social behavior, self-protection\n",
    "- **Bilingual**: Model responds in the language of the user's question\n",
    "\n",
    "### Model\n",
    "- **Base Model**: Google Gemma 3 4B Instruct (`google/gemma-3-4b-it`)\n",
    "- **Size**: 4 billion parameters - more capable than 2B models\n",
    "- **Architecture**: Latest Gemma 3 instruction-tuned architecture\n",
    "- **Languages**: English and Kinyarwanda fluency\n",
    "\n",
    "### Training Strategy\n",
    "- **QLoRA (4-bit)**: Reduces memory usage significantly\n",
    "- **Identity Oversampling**: Identity data repeated 8x to ensure the model learns its name and creator\n",
    "- **Balanced Languages**: Both English and Kinyarwanda data included\n",
    "- **Language Matching**: Model trained to respond in the user's language\n",
    "\n",
    "### Memory Management\n",
    "If you encounter OOM (Out of Memory) errors:\n",
    "- Reduce `per_device_train_batch_size` to 2 or 1\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- Use Google Colab Pro for more GPU memory\n",
    "- Close other GPU applications\n",
    "\n",
    "### GGUF Conversion\n",
    "- **Q4_K_M Quantization**: Best balance of size and quality\n",
    "- **File Size**: ~2-3GB for Q4_K_M (4B model)\n",
    "\n",
    "### Deployment\n",
    "- **Ollama**: Primary deployment method\n",
    "- **Model Name**: `nganiriza`\n",
    "- **Languages**: Responds in English or Kinyarwanda based on user input\n",
    "\n",
    "### Ethical Considerations\n",
    "1. **Not a Replacement**: Clearly states it's not professional therapy\n",
    "2. **Crisis Resources**: Includes Rwanda and international crisis lines\n",
    "3. **Evidence-Based**: All advice follows current best practices\n",
    "4. **Non-Judgmental**: Maintains supportive, accepting tone\n",
    "5. **Boundaries**: Knows limitations and encourages professional help\n",
    "6. **Cultural Sensitivity**: Respects Rwandan culture and values"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
